{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1ed0152-e3b5-4fcb-a8ef-1fdb70b92889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "69c06f7c-fad1-4652-aad7-47553e682878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры\n",
    "batch_size = 8  # Уменьшено\n",
    "embedding_size = 128\n",
    "hidden_size = 350\n",
    "num_layers = 1\n",
    "dropout = 0.2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "clip = 1\n",
    "min_freq = 1  # Снижено\n",
    "test_size = 0.2\n",
    "random_seed = 42\n",
    "max_length = 10  # Уменьшено\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "81a8965f-fc8a-49cc-89be-353386223347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Данные (примеры)\n",
    "english_sentences = [\n",
    "    \"Hello world\",\n",
    "    \"How are you?\",\n",
    "    \"What is your name?\",\n",
    "    \"I am learning machine translation\",\n",
    "    \"This is a simple example\",\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Coding is fun\",\n",
    "    \"Machine learning is interesting\",\n",
    "    \"Python is a powerful language\",\n",
    "    \"I love data science\",\n",
    "    \"What can you tell us about yourself?\",\n",
    "    \"I can program in many languages\",\n",
    "    \"Artificial intelligence helps me\"\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "russian_sentences = [\n",
    "    \"Привет мир\",\n",
    "    \"Как дела?\",\n",
    "    \"Как тебя зовут?\",\n",
    "    \"Я изучаю машинный перевод\",\n",
    "    \"Это простой пример\",\n",
    "    \"Быстрая коричневая лиса перепрыгивает через ленивую собаку\",\n",
    "    \"Программировать это весело\",\n",
    "    \"Машинное обучение это интересно\",\n",
    "    \"Python это мощный язык\",\n",
    "    \"Я люблю науку о данных\",\n",
    "    \"Что ты можешь о себе сообщить?\",\n",
    "    \"Я умею программировать на многих языках\",\n",
    "    \"Мне помогает искусственный интеллект\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f742073c-097b-4bdf-a4eb-ffb3811f23f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Создание словарей\n",
    "def create_vocabularies(english_texts, russian_texts, min_freq=1): # min_freq: #Минимальная частота, с которой слово должно встречаться в корпусе, \n",
    "    # чтобы быть включенным в словарь (по умолчанию 1\n",
    "    english_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "    russian_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "#  '<pad>': Токен для паддинга (используется для выравнивания последовательностей разной длины).  Ему присваивается индекс 0\n",
    "#  '<sos>': Токен начала последовательности (start-of-sequence). Ему присваивается индекс 1.\n",
    "#  '<eos>': Токен конца последовательности (end-of-sequence). Ему присваивается индекс 2\n",
    "# '<unk>': Токен для неизвестных слов (unknown).  Ему присваивается индекс 3    \n",
    "    english_word_counts = {}\n",
    "    russian_word_counts = {}\n",
    "# пустые английские и раууские словари для подсчета частоты каждого слова \n",
    "    for text in english_texts:\n",
    "        for token in nltk.word_tokenize(text):\n",
    "            english_word_counts[token] = english_word_counts.get(token, 0) + 1 #  Обновляет счетчик для текущего токена в словаре english_word_counts\n",
    "                                                                               # и  сохраняет обновленный счетчик в словаре для текущего токена             \n",
    "    for text in russian_texts:\n",
    "        for token in nltk.word_tokenize(text):\n",
    "            russian_word_counts[token] = russian_word_counts.get(token, 0) + 1  # аналогично предыдущему циклу для русского\n",
    "\n",
    "    english_index = len(english_vocab)\n",
    "    for word, count in english_word_counts.items():\n",
    "        if count >= min_freq:                                      # фильтруем редкие слова\n",
    "            english_vocab[word] = english_index\n",
    "            english_index += 1\n",
    "\n",
    "    russian_index = len(russian_vocab)\n",
    "    for word, count in russian_word_counts.items():\n",
    "        if count >= min_freq:\n",
    "            russian_vocab[word] = russian_index\n",
    "            russian_index += 1\n",
    "\n",
    "    return english_vocab, russian_vocab\n",
    "\n",
    "english_vocab, russian_vocab = create_vocabularies(english_sentences, russian_sentences, min_freq)\n",
    "russian_vocab_size = len(russian_vocab)  # Запоминаем размер словаря\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a2e9bdb2-cf54-4399-95c0-2ac16197d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Dataset\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, english_texts, russian_texts, english_vocab, russian_vocab, max_length=10):\n",
    "        self.english_texts = english_texts\n",
    "        self.russian_texts = russian_texts\n",
    "        self.english_vocab = english_vocab\n",
    "        self.russian_vocab = russian_vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        english_text = self.english_texts[idx]\n",
    "        russian_text = self.russian_texts[idx]\n",
    "\n",
    "        # Токенизация с помощью nltk\n",
    "        english_tokens = nltk.word_tokenize(english_text)[:self.max_length]\n",
    "        russian_tokens = nltk.word_tokenize(russian_text)[:self.max_length]\n",
    "\n",
    "        # Добавление токенов <sos> и <eos>\n",
    "        english_tokens = ['<sos>'] + english_tokens + ['<eos>']\n",
    "        russian_tokens = ['<sos>'] + russian_tokens + ['<eos>']\n",
    "\n",
    "        # Численное представление\n",
    "        english_indices = [self.english_vocab.get(token, self.english_vocab['<unk>']) for token in english_tokens]\n",
    "        russian_indices = [self.russian_vocab.get(token, self.russian_vocab['<unk>']) for token in russian_tokens]\n",
    "\n",
    "        # Паддинг\n",
    "        english_indices = english_indices + [self.english_vocab['<pad>']] * (self.max_length + 2 - len(english_indices))\n",
    "        russian_indices = russian_indices + [self.russian_vocab['<pad>']] * (self.max_length + 2 - len(russian_indices))\n",
    "\n",
    "        return torch.tensor(english_indices), torch.tensor(russian_indices)\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "english_train, english_test, russian_train, russian_test = train_test_split(\n",
    "    english_sentences, russian_sentences, test_size=test_size, random_state=random_seed\n",
    ")\n",
    "\n",
    "train_dataset = TranslationDataset(english_train, russian_train, english_vocab, russian_vocab, max_length)\n",
    "test_dataset = TranslationDataset(english_test, russian_test, english_vocab, russian_vocab, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e6a2054f-7070-4143-ae07-069f13153424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "def collate_fn(batch):\n",
    "    english_batch, russian_batch = [], []\n",
    "    for en_sample, ru_sample in batch:\n",
    "        english_batch.append(en_sample)\n",
    "        russian_batch.append(ru_sample)\n",
    "\n",
    "    english_batch = torch.stack(english_batch)\n",
    "    russian_batch = torch.stack(russian_batch)\n",
    "    return english_batch, russian_batch\n",
    "\n",
    "train_iterator = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_iterator = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c732fde2-d1ce-49fd-8149-08ba977da8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder/Decoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embedding_size, hidden_size, num_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = x.unsqueeze(1)\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.fc(output.squeeze(1))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        batch_size = source.shape[0]\n",
    "        target_len = target.shape[1]\n",
    "        target_vocab_size = self.decoder.fc.out_features  # Get output vocab size correctly\n",
    "\n",
    "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(source)\n",
    "\n",
    "        x = target[:, 0].to(self.device)  # <sos> token\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            outputs[:, t] = output\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            x = (target[:, t] if teacher_force else top1).to(self.device)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c3d06140-c9da-423e-994b-4935af258528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение\n",
    "def train(model, iterator, optimizer, criterion, clip, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, (source, target) in enumerate(iterator):\n",
    "        source, target = source.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(source, target)\n",
    "\n",
    "        batch_size, seq_len, vocab_size = output.shape # Correct way to get vocab_size\n",
    "        output = output[:, 1:].reshape(-1, vocab_size)\n",
    "        target = target[:, 1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c35bf549-6740-4c6a-a404-4fb8a73eea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Перевод и оценка\n",
    "def translate_sentence(model, sentence, english_vocab, russian_vocab, device, max_length=10):\n",
    "    model.eval()\n",
    "    tokens = nltk.word_tokenize(sentence)[:max_length]\n",
    "    tokens = ['<sos>'] + tokens + ['<eos>']\n",
    "    indices = [english_vocab.get(token, english_vocab['<unk>']) for token in tokens]\n",
    "    indices = indices + [english_vocab['<pad>']] * (max_length + 2 - len(indices))\n",
    "\n",
    "    source = torch.tensor(indices).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden, cell = model.encoder(source)\n",
    "        outputs = [russian_vocab['<sos>']]\n",
    "        x = torch.tensor([russian_vocab['<sos>']]).to(device)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            output, hidden, cell = model.decoder(x, hidden, cell)\n",
    "            predicted_token = output.argmax(1).item()\n",
    "\n",
    "            outputs.append(predicted_token)\n",
    "\n",
    "            if predicted_token == russian_vocab['<eos>']:\n",
    "                break\n",
    "            x = torch.tensor([predicted_token]).to(device)\n",
    "    predicted_words = [key for key, value in russian_vocab.items() if value in outputs]\n",
    "\n",
    "    return ' '.join(predicted_words[1:-1])\n",
    "\n",
    "def calculate_bleu(model, data, english_vocab, russian_vocab, device, max_length=10):\n",
    "    bleu_scores = []\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    for english, russian in zip(data.english_texts, data.russian_texts):\n",
    "        candidate = translate_sentence(model, english, english_vocab, russian_vocab, device, max_length)\n",
    "        reference = nltk.word_tokenize(russian)\n",
    "        candidate_list = candidate.split()\n",
    "\n",
    "        try:\n",
    "            bleu_score = sentence_bleu([reference], candidate_list, smoothing_function=smoothing)\n",
    "        except ZeroDivisionError:\n",
    "            bleu_score = 0\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "    return sum(bleu_scores) / len(bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3c38262e-332c-47ec-a348-c39e4025cdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха: 01, Потери: 3.9469\n",
      "Эпоха: 02, Потери: 3.8347\n",
      "Эпоха: 03, Потери: 3.6404\n",
      "Эпоха: 04, Потери: 3.6049\n",
      "Эпоха: 05, Потери: 3.1442\n",
      "Эпоха: 06, Потери: 3.0035\n",
      "Эпоха: 07, Потери: 2.8918\n",
      "Эпоха: 08, Потери: 2.9180\n",
      "Эпоха: 09, Потери: 2.7944\n",
      "Эпоха: 10, Потери: 2.4644\n",
      "Эпоха: 11, Потери: 2.2333\n",
      "Эпоха: 12, Потери: 2.1550\n",
      "Эпоха: 13, Потери: 2.1841\n",
      "Эпоха: 14, Потери: 2.1250\n",
      "Эпоха: 15, Потери: 1.8878\n",
      "Эпоха: 16, Потери: 1.6246\n",
      "Эпоха: 17, Потери: 1.8827\n",
      "Эпоха: 18, Потери: 1.6712\n",
      "Эпоха: 19, Потери: 1.4884\n",
      "Эпоха: 20, Потери: 1.4869\n",
      "Эпоха: 21, Потери: 1.0557\n",
      "Эпоха: 22, Потери: 1.1349\n",
      "Эпоха: 23, Потери: 1.0094\n",
      "Эпоха: 24, Потери: 1.7777\n",
      "Эпоха: 25, Потери: 0.9319\n",
      "Эпоха: 26, Потери: 0.8641\n",
      "Эпоха: 27, Потери: 0.6909\n",
      "Эпоха: 28, Потери: 0.7835\n",
      "Эпоха: 29, Потери: 0.5934\n",
      "Эпоха: 30, Потери: 0.9792\n",
      "Эпоха: 31, Потери: 0.4916\n",
      "Эпоха: 32, Потери: 0.3951\n",
      "Эпоха: 33, Потери: 0.5188\n",
      "Эпоха: 34, Потери: 0.4341\n",
      "Эпоха: 35, Потери: 0.3372\n",
      "Эпоха: 36, Потери: 0.5186\n",
      "Эпоха: 37, Потери: 0.6933\n",
      "Эпоха: 38, Потери: 0.5262\n",
      "Эпоха: 39, Потери: 0.7722\n",
      "Эпоха: 40, Потери: 0.3423\n",
      "Эпоха: 41, Потери: 0.8969\n",
      "Эпоха: 42, Потери: 0.4465\n",
      "Эпоха: 43, Потери: 0.8097\n",
      "Эпоха: 44, Потери: 0.1959\n",
      "Эпоха: 45, Потери: 0.1944\n",
      "Эпоха: 46, Потери: 0.6014\n",
      "Эпоха: 47, Потери: 0.2115\n",
      "Эпоха: 48, Потери: 0.1783\n",
      "Эпоха: 49, Потери: 0.2250\n",
      "Эпоха: 50, Потери: 0.1476\n",
      "BLEU score на тестовом наборе: 0.0162\n",
      "English: How are you?\n",
      "Russian: <eos> Как дела\n"
     ]
    }
   ],
   "source": [
    "# 9. Main\n",
    "if __name__ == '__main__':\n",
    "    # Определение моделей\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    encoder = Encoder(len(english_vocab), embedding_size, hidden_size, num_layers, dropout).to(device)\n",
    "    decoder = Decoder(russian_vocab_size, embedding_size, hidden_size, num_layers, dropout).to(device) # Use correct vocab size\n",
    "    model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "    # Оптимизатор и функция потерь\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    # Обучение модели\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_iterator, optimizer, criterion, clip, device)\n",
    "        print(f'Эпоха: {epoch + 1:02}, Потери: {train_loss:.4f}')\n",
    "\n",
    "    # Оценка BLEU\n",
    "    test_bleu = calculate_bleu(model, test_dataset, english_vocab, russian_vocab, device, max_length)\n",
    "    print(f'BLEU score на тестовом наборе: {test_bleu:.4f}')\n",
    "\n",
    "    # Пример перевода\n",
    "    example_sentence = \"How are you?\"\n",
    "    translated_sentence = translate_sentence(model, example_sentence, english_vocab, russian_vocab, device, max_length)\n",
    "    print(f\"English: {example_sentence}\")\n",
    "    print(f\"Russian: {translated_sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e67768-d3e7-4a02-89e2-937981468f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
